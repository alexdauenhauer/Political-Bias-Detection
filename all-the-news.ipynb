{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to /home/alex/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time, os, pickle, re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, names, opinion_lexicon\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from three separate csv files\n",
    "df1 = pd.read_csv(r'../all-the-news/articles1.csv')\n",
    "df2 = pd.read_csv(r'../all-the-news/articles2.csv')\n",
    "df3 = pd.read_csv(r'../all-the-news/articles3.csv')\n",
    "df = df1.append(df2).append(df3)\n",
    "\n",
    "# reindex the concatenated dataframe\n",
    "df.index = range(df.shape[0])\n",
    "\n",
    "# add the title sentence to the content body\n",
    "df['content'] = df.content + ' ' + df.title\n",
    "\n",
    "# keep only the publication and content columns and delete the other DFs for memory conservation\n",
    "df = df.loc[:,['publication','content']]\n",
    "del df1, df2, df3\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142568, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop missing data\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime (min): 3.253500250975291\n"
     ]
    }
   ],
   "source": [
    "# parse the articles into sentences\n",
    "start = time.time()\n",
    "df['sentences'] = df.content.apply(sent_tokenize)\n",
    "print('runtime (min):', (time.time() - start) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply bias labels, sourced from mediabiasfactcheck.com based on publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual labels from MBFC\n",
    "bias_dict = {\n",
    "    'New York Times': 'left-center',\n",
    "    'Breitbart': 'extreme-right',\n",
    "    'CNN': 'left',\n",
    "    'Business Insider': 'left-center',\n",
    "    'Atlantic': 'left-center',\n",
    "    'Fox News': 'right',\n",
    "    'Talking Points Memo': 'left',\n",
    "    'Buzzfeed News': 'left-center',\n",
    "    'National Review': 'right',\n",
    "    'New York Post': 'right-center',\n",
    "    'Guardian': 'left-center',\n",
    "    'NPR': 'left-center',\n",
    "    'Reuters': 'neutral',\n",
    "    'Vox': 'left',\n",
    "    'Washington Post': 'left-center'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times 1\n",
      "Breitbart -1\n",
      "CNN 1\n",
      "Business Insider 1\n",
      "Atlantic 1\n",
      "Fox News -1\n",
      "Talking Points Memo 1\n",
      "Buzzfeed News 1\n",
      "National Review -1\n",
      "New York Post -1\n",
      "Guardian 1\n",
      "NPR 1\n",
      "Reuters 0\n",
      "Vox 1\n",
      "Washington Post 1\n"
     ]
    }
   ],
   "source": [
    "# simplified labels\n",
    "for k,v in bias_dict.items():\n",
    "    if 'left' in v:\n",
    "        # 1 for liberal\n",
    "        bias_dict[k] = 1\n",
    "    elif 'right' in v:\n",
    "        # -1 for conservative\n",
    "        bias_dict[k] = -1\n",
    "    else:\n",
    "        # 0 for neutral\n",
    "        bias_dict[k] = 0\n",
    "for k,v in bias_dict.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication', 'content', 'label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a label column\n",
    "df['label'] = [bias_dict[p] for p in df.publication.values]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save partial dataframe for document classification task later\n",
    "with open(r'../data/newsFullDocs.pickle', 'wb') as f:\n",
    "    pickle.dump(df.loc[:,['label','sentences']], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop sentences column for memory conservation\n",
    "df = df.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Bias Detector n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a subset of publishers that have the most extreme MBFC labels, collect the most common trigrams and bigrams for both the conservative and liberal labels. Filter out stopwords on all n-grams and require that bigrams contain at least one \"opinion\" word from the NLTK `opinion_lexicon` corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'york',\n",
       " 'times',\n",
       " 'breitbart',\n",
       " 'cnn',\n",
       " 'business',\n",
       " 'insider',\n",
       " 'atlantic',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'talking',\n",
       " 'points',\n",
       " 'memo',\n",
       " 'buzzfeed',\n",
       " 'news',\n",
       " 'national',\n",
       " 'review',\n",
       " 'new',\n",
       " 'york',\n",
       " 'post',\n",
       " 'guardian',\n",
       " 'npr',\n",
       " 'reuters',\n",
       " 'vox',\n",
       " 'washington',\n",
       " 'post']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add unigrams from publisher names to list of stopwords\n",
    "pubs = []\n",
    "for p in df.publication.unique():\n",
    "    pubs.extend(p.lower().split())\n",
    "pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Breitbart', 'CNN', 'Fox News', 'Talking Points Memo', 'Vox'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select subset of publishers with most extreme MPFC labels for n-gram selection\n",
    "publishers = ['Breitbart', 'CNN', 'Fox News', 'Talking Points Memo','Vox']\n",
    "idx = [i for i in range(df.shape[0]) if df.publication.iloc[i] in publishers]\n",
    "df = df.iloc[idx]\n",
    "df.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49783, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the size of the subset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>23781</td>\n",
       "      <td>23781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>11488</td>\n",
       "      <td>11488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>4354</td>\n",
       "      <td>4354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>5213</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>4947</td>\n",
       "      <td>4947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     content  label\n",
       "publication                        \n",
       "Breitbart              23781  23781\n",
       "CNN                    11488  11488\n",
       "Fox News                4354   4354\n",
       "Talking Points Memo     5213   5213\n",
       "Vox                     4947   4947"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the balance of content per publisher\n",
    "df.groupby('publication').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>28135</td>\n",
       "      <td>28135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21648</td>\n",
       "      <td>21648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication  content\n",
       "label                      \n",
       "-1           28135    28135\n",
       " 1           21648    21648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how balanced the labels are\n",
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into liberal and conservative\n",
    "lib = df.loc[df.label == 1]\n",
    "con = df.loc[df.label == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom stopwords list by adding custom stops found empirically from data\n",
    "stops = [w.lower() for w in stopwords.words('english')] + \\\n",
    "['trump', 'good','great','bad','pretty','covering','writer',\n",
    " 'author','like','news','follow','tv','said','could','would',\n",
    " 'really','best','journalist','journalists','commentator',\n",
    " 'affiliate','told','reporter','reporters','siriusxm','radio',\n",
    " 'columnist','contributed','bestselling','battleground','weekend',\n",
    " 'seven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add English names to list of stopwords\n",
    "men = [w.lower() for w in names.words('male.txt')]\n",
    "women = [w.lower() for w in names.words('female.txt')]\n",
    "stopNames = men + women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the stops list\n",
    "stops.extend(pubs)\n",
    "stops.extend(stopNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of opinion words\n",
    "ops = opinion_lexicon.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sets for each list\n",
    "stops = set(stops)\n",
    "ops = set(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7803, 6786)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stops), len(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to select n-grams\n",
    "def getNgrams(text, stops=None, ops=None, n=2):\n",
    "    '''return all n-grams from sentence that meet criteria in a Counter'''\n",
    "    \n",
    "    def criteria(word):\n",
    "        # determine if a word meets the criteria for keeping that word\n",
    "        if stops:\n",
    "            if len(word) > 2 and not word.isnumeric() and word not in stops:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    # intialize counter\n",
    "    ngrams = Counter()\n",
    "    \n",
    "    # tokenize sentence into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # throw out sentences with less than n words\n",
    "    if len(words) < n:\n",
    "        return ngrams\n",
    "    \n",
    "    # filter punctuation and single letter words\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    # add n-grams to Counter\n",
    "    for i in range(len(words) - (n-1)):\n",
    "        gram = ' '.join(words[i:i+n])\n",
    "        if all([criteria(w) for w in words[i:i+n]]):\n",
    "            if ops:\n",
    "                if any([w in ops for w in words[i:i+n]]):\n",
    "                    ngrams[gram] += 1\n",
    "            else:\n",
    "                ngrams[gram] += 1\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21648/21648 [03:16<00:00, 110.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# get list of all liberal trigrams\n",
    "libTrigrams = Counter()\n",
    "for pub,content,_ in tqdm(lib.values):\n",
    "    sents = sent_tokenize(content)\n",
    "    for text in sents:\n",
    "        libTrigrams.update(getNgrams(text, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21648/21648 [03:24<00:00, 105.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# get list of all liberal bigrams\n",
    "libBigrams = Counter()\n",
    "for pub,content,_ in tqdm(lib.values):\n",
    "    sents = sent_tokenize(content)\n",
    "    for text in sents:\n",
    "        libBigrams.update(getNgrams(text, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28135/28135 [02:54<00:00, 161.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# get list of all conservative trigrams\n",
    "conTrigrams = Counter()\n",
    "for pub,content,_ in tqdm(con.values):\n",
    "    sents = sent_tokenize(content)\n",
    "    for text in sents:\n",
    "        conTrigrams.update(getNgrams(text, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28135/28135 [02:49<00:00, 166.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# get list of all consrvative bigrams\n",
    "conBigrams = Counter()\n",
    "for pub,content,_ in tqdm(con.values):\n",
    "    sents = sent_tokenize(content)\n",
    "    for text in sents:\n",
    "        conBigrams.update(getNgrams(text, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1000 most common liberal and conservative n-grams\n",
    "commonConTrigrams = [b[0] for b in conTrigrams.most_common()[:1000]]\n",
    "commonLibTrigrams = [b[0] for b in libTrigrams.most_common()[:1000]]\n",
    "commonConBigrams = [b[0] for b in conBigrams.most_common()[:1000]]\n",
    "commonLibBigrams = [b[0] for b in libBigrams.most_common()[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter specific bigrams that passed criteria requirements but do not indicate bias\n",
    "remove_bigrams = ['doctor strange','fury road','gold medals','gold medal',\n",
    "                  'mad men','stranger things','little lies','lose weight',\n",
    "                  'world champion','premier league','walking dead',\n",
    "                  'weight loss','grand slam','science fiction','right now.',\n",
    "                  'open champion','associated press','world heavyweight',\n",
    "                  'wonder woman','last month','top stories','fast facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the set difference for liberal and conservative n-grams and remove any n-grams containing bigrams from from remove_bigrams\n",
    "libTrigrams_filtered = [(w, libTrigrams[w]) for w in commonLibTrigrams if w not in commonConTrigrams and not any([b in w for b in remove_bigrams])]\n",
    "libBigrams_filtered = [(w, libBigrams[w]) for w in commonLibBigrams if w not in commonConBigrams and w not in remove_bigrams]\n",
    "conTrigrams_filtered = [(w, conTrigrams[w]) for w in commonConTrigrams if w not in commonLibTrigrams and not any([b in w for b in remove_bigrams])]\n",
    "conBigrams_filtered = [(w, conBigrams[w]) for w in commonConBigrams if w not in commonLibBigrams and w not in remove_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 501, 426, 441)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of each set\n",
    "len(libTrigrams_filtered),len(conTrigrams_filtered),len(libBigrams_filtered),len(conBigrams_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 LIBERAL TRIGRAMS:\n",
      "\n",
      "senior administration official\n",
      "greenhouse gas emissions\n",
      "north korean leader\n",
      "federal civil rights\n",
      "provocative narrative essays\n",
      "clean air act\n",
      "health care policy\n",
      "republican health care\n",
      "gop health care\n",
      "civil rights laws\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 liberal trigrams\n",
    "print('TOP 10 LIBERAL TRIGRAMS:\\n')\n",
    "for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 LIBERAL BIGRAMS:\n",
      "\n",
      "opioid epidemic\n",
      "health reform\n",
      "lethal injection\n",
      "healthy people\n",
      "racial bias\n",
      "budget reconciliation\n",
      "chronic pain\n",
      "lead poisoning\n",
      "intelligence committees\n",
      "rights advocates\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 liberal bigrams\n",
    "print('TOP 10 LIBERAL BIGRAMS:\\n')\n",
    "for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 CONSERVATIVE TRIGRAMS:\n",
      "\n",
      "jerusalem bureau chief\n",
      "border patrol agent\n",
      "social justice warriors\n",
      "black panther party\n",
      "cartel chronicles project\n",
      "refugee resettlement program\n",
      "prison sentence commuted\n",
      "god less america\n",
      "real clear politics\n",
      "face certain death\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 conservative trigrams\n",
    "print('TOP 10 CONSERVATIVE TRIGRAMS:\\n')\n",
    "for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 CONSERVATIVE BIGRAMS:\n",
      "\n",
      "illegal aliens\n",
      "illegal alien\n",
      "illegal immigrant\n",
      "migrant crisis\n",
      "patriot channel\n",
      "hard truths\n",
      "twin falls\n",
      "islamic terror\n",
      "snarky opinions\n",
      "dangerous faggot\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 conservative bigrams\n",
    "print('TOP 10 CONSERVATIVE BIGRAMS:\\n')\n",
    "for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the n-grams and drop the count data\n",
    "libGrams = [k for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:100]]\n",
    "conGrams = [k for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off the n-grams for loading later\n",
    "with open('../data/libGrams_news6.pickle', 'wb') as f:\n",
    "    pickle.dump(libGrams, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/conGrams_news6.pickle', 'wb') as f:\n",
    "    pickle.dump(conGrams, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter sentences for Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load liberal and conservative n-grams for sentence filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senior administration official', 'greenhouse gas emissions', 'north korean leader', 'federal civil rights', 'provocative narrative essays']\n",
      "['jerusalem bureau chief', 'border patrol agent', 'social justice warriors', 'black panther party', 'cartel chronicles project']\n"
     ]
    }
   ],
   "source": [
    "# load the n-grams\n",
    "with open('../data/libGrams_news6.pickle', 'rb') as f:\n",
    "    libGrams = pickle.load(f)\n",
    "    print(libGrams[:5])\n",
    "with open('../data/conGrams_news6.pickle', 'rb') as f:\n",
    "    conGrams = pickle.load(f)\n",
    "    print(conGrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Data and test sentence filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r'../all-the-news/articles1.csv')\n",
    "df2 = pd.read_csv(r'../all-the-news/articles2.csv')\n",
    "df3 = pd.read_csv(r'../all-the-news/articles3.csv')\n",
    "df = df1.append(df2).append(df3)\n",
    "df.index = range(df.shape[0])\n",
    "df['content'] = df.content + ' ' + df.title\n",
    "df = df.loc[:,['publication','content']]\n",
    "del df1, df2, df3\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bias dict again\n",
    "bias_dict = {\n",
    "    'New York Times': 'left-center',\n",
    "    'Breitbart': 'extreme-right',\n",
    "    'CNN': 'left',\n",
    "    'Business Insider': 'left-center',\n",
    "    'Atlantic': 'left-center',\n",
    "    'Fox News': 'right',\n",
    "    'Talking Points Memo': 'left',\n",
    "    'Buzzfeed News': 'left-center',\n",
    "    'National Review': 'right',\n",
    "    'New York Post': 'right-center',\n",
    "    'Guardian': 'left-center',\n",
    "    'NPR': 'left-center',\n",
    "    'Reuters': 'neutral',\n",
    "    'Vox': 'left',\n",
    "    'Washington Post': 'left-center'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times 1\n",
      "Breitbart -1\n",
      "CNN 1\n",
      "Business Insider 1\n",
      "Atlantic 1\n",
      "Fox News -1\n",
      "Talking Points Memo 1\n",
      "Buzzfeed News 1\n",
      "National Review -1\n",
      "New York Post -1\n",
      "Guardian 1\n",
      "NPR 1\n",
      "Reuters 0\n",
      "Vox 1\n",
      "Washington Post 1\n"
     ]
    }
   ],
   "source": [
    "# simplified labels\n",
    "for k,v in bias_dict.items():\n",
    "    if 'left' in v:\n",
    "        bias_dict[k] = 1\n",
    "    elif 'right' in v:\n",
    "        bias_dict[k] = -1\n",
    "    else:\n",
    "        bias_dict[k] = 0\n",
    "for k,v in bias_dict.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = [bias_dict[p] for p in df.publication.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to determine if sentence should be kept or thrown out\n",
    "def keepSentence(label, text):\n",
    "    # remove sentences with less than 6 words\n",
    "    if len(text.split()) < 6:\n",
    "        return False\n",
    "    \n",
    "    # get trigrams and bigrams\n",
    "    ngrams = list(getNgrams(text,n=2).keys()) + list(getNgrams(text,n=3).keys())\n",
    "    \n",
    "    # keep only n-grams that appear in detector lists\n",
    "    libNgramSet = set(ngrams).intersection(libGrams)\n",
    "    conNgramSet = set(ngrams).intersection(conGrams)\n",
    "    \n",
    "    # determine whether to keep the sentence\n",
    "    if label == 1:\n",
    "        return libNgramSet\n",
    "    elif label == -1:\n",
    "        return conNgramSet\n",
    "    else:\n",
    "        if libNgramSet or conNgramSet:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the dataframe\n",
    "def filterText(df):\n",
    "    filteredText = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        # grab the label and tokenize each sentence in the doc\n",
    "        label = df.label.iloc[i]\n",
    "        sents = sent_tokenize(df.content.iloc[i])\n",
    "        \n",
    "        # determine whether to keep the sentences\n",
    "        for text in sents:\n",
    "            ngrams = keepSentence(label,text)\n",
    "            \n",
    "            # if the returned n-gram set is not empty, add it to the list\n",
    "            if ngrams:\n",
    "                filteredText.append((label, text, ngrams))\n",
    "            \n",
    "    return pd.DataFrame(filteredText, columns=['label','text','ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# test on sample\n",
    "sample = df.sample(100)\n",
    "df2 = filterText(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  ngrams\n",
       "label              \n",
       "-1       11      11\n",
       " 0      274     274\n",
       " 1       24      24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142570/142570 [38:55<00:00, 47.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>16745</td>\n",
       "      <td>16745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277751</td>\n",
       "      <td>277751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28230</td>\n",
       "      <td>28230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  ngrams\n",
       "label                \n",
       "-1      16745   16745\n",
       " 0     277751  277751\n",
       " 1      28230   28230"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the full dataset\n",
    "df2 = filterText(df)\n",
    "df2.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete original set for memory conservation\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = round(min(df2[df2.label == -1].shape[0], df2[df2.label == 1].shape[0]),-3)\n",
    "n = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a sample of values from each label for label balancing\n",
    "s1 = df2.loc[df2.label == -1].sample(n)\n",
    "s2 = df2.loc[df2.label == 1].sample(n)\n",
    "s3 = df2.loc[df2.label == 0].sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 3), 48000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new df with balanced labels\n",
    "df2 = s1.append(s2).append(s3)\n",
    "df2.shape, n*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off the filtered dataset\n",
    "with open(r'../data/filteredNews6.pickle', 'wb') as f:\n",
    "    pickle.dump(df2.loc[:,['label','text']], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload saved data and remove escape characters\n",
    "Found that escape characters were causing trouble with model predictions so we removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/filteredNews6.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@CookPolitical\\r  —   Dave Wasserman (@Redistrict) March 18, 2016       Dave Wasserman is the US House editor of the Cook Political Report, the gold standard for granular analysis of congressional races.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of escape characters in text causing issues\n",
    "s = df.text.iloc[30794]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  —   Dave Wasserman (@Redistrict) March 18, 2016       Dave Wasserman is the US House editor of the Cook Political Report, the gold standard for granular analysis of congressional races.\n"
     ]
    }
   ],
   "source": [
    "# showing the break in the sentence from the carriage return\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any escape characters and create filtered text column\n",
    "pattern = re.compile('[\\t\\r\\n\\f\\v]')\n",
    "newText = []\n",
    "for i,t in enumerate(df.text):\n",
    "    newText.append(re.sub(pattern, '', t))\n",
    "df['newText'] = newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unfiltered text\n",
    "df = df.drop('text', axis=1)\n",
    "df.columns = ['label','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off filtered dataset\n",
    "with open(r'../data/filteredNews6.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'../data/filteredNews6.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17651 Here is Dave Wasserman, who writes for the Cook Political Report and FiveThirtyEight, making the argument:   Jill Stein is now officially the Ralph Nader of 2016. \n",
      "\n",
      "22942 And Bernie Sanders’s path to victory depends on winning the voters Hillary Clinton won in 2008,” says Dave Wasserman, a political analyst at the nonpartisan Cook Political Report. \n",
      "\n",
      "28648 One of the top guys I know to follow for this kind of analysis on Twitter is Dave Wasserman of the Cook Political Report. \n",
      "\n",
      "30794 @CookPolitical  —   Dave Wasserman (@Redistrict) March 18, 2016       Dave Wasserman is the US House editor of the Cook Political Report, the gold standard for granular analysis of congressional races. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that it worked...\n",
    "pattern = 'Dave Wasserman'\n",
    "for i,t in enumerate(df.text):\n",
    "    if pattern in t:\n",
    "        print(i,t,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sample sentences for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the prevalence of substance abuse in the United States, An estimated 20. \n",
      "\n",
      "The episode was just one of several examples that highlighted the relationship Pruitt and his top aides maintained with Devon Energy and the oil and gas industry during his time as Oklahoma attorney general. \n",
      "\n",
      "Schleicher says there’s also great promise in the Common Core State Standards, adopted by over 40 states, and which   Donald Trump has called “a total disaster. \n",
      "\n",
      "“I continue to have real concerns about the Medicaid policies in this bill, especially those that impact drug treatment at a time when Ohio is facing an opioid epidemic,” Portman said this week, citing the issue that became his personal rallying cry last year. \n",
      "\n",
      "It called on President Salva Kiir and his rival Vice President Riek Machar to control their respective forces, prevent the spread of violence and genuinely commit themselves to the implementation of a ceasefire and peace agreement. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df[df.label == 1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Judd’s comments came during an interview with Fox Business Channel’s Stuart Varney where he addressed the dramatic impact the Trump Administration has already had on reducing illegal border crossings. \n",
      "\n",
      "A second picture from the same parade showed a man dressed in the rags of a homeless man carrying a sign reading “if only I was a migrant” reports   Russia’s Federal Security Service (FSB) announced Monday that it had arrested seven members of an Islamic State terror cell who were allegedly planning major jihadist attacks on Moscow and St. Petersburg. \n",
      "\n",
      "Herridge joined FNC in 1996 as a   correspondent. \n",
      "\n",
      "Reports note that the gang rape might never have come to light if one of Burrows’ fellow gang members hadn’t bragged about the rape to his   and showed him the “funny” video recording of the rape, which he had saved on his cellphone. \n",
      "\n",
      "” “Expect our civil disobedience to continue,” he added. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df[df.label == -1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”I say it and I’m going to keep saying it and some people say: ’Wow that makes sense’ and some people say: ’That’s not very nice,’” Trump said. \n",
      "\n",
      "Historically, those taxes have been kept low by the government’s aversion to raising rates. \n",
      "\n",
      "”It couldn’t be any worse, could it?” A spokeswoman for Clinton said Wednesday’s report was ”disturbing.” ”These reports suggest that he lied on the debate stage and that the disgusting behavior he bragged about in the tape is more than just words,” said Jennifer Palmieri, a spokeswoman for the Clinton campaign. \n",
      "\n",
      "Last year, China signed an agreement with Pakistan for the sale of eight submarines. \n",
      "\n",
      "English, who was the sole contender for the job, announced Social Housing Minister Paula Bennett as the deputy leader, following a special caucus meeting of the ruling   National Party. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df[df.label == 0].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

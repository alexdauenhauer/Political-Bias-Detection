{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to /home/alex/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time, os, pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, names, opinion_lexicon\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r'../all-the-news/articles1.csv')\n",
    "df2 = pd.read_csv(r'../all-the-news/articles2.csv')\n",
    "df3 = pd.read_csv(r'../all-the-news/articles3.csv')\n",
    "df = df1.append(df2).append(df3)\n",
    "df.index = range(df.shape[0])\n",
    "df['content'] = df.content + ' ' + df.title\n",
    "df = df.loc[:,['publication','content']]\n",
    "del df1, df2, df3\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'york',\n",
       " 'times',\n",
       " 'breitbart',\n",
       " 'cnn',\n",
       " 'business',\n",
       " 'insider',\n",
       " 'atlantic',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'talking',\n",
       " 'points',\n",
       " 'memo',\n",
       " 'buzzfeed',\n",
       " 'news',\n",
       " 'national',\n",
       " 'review',\n",
       " 'new',\n",
       " 'york',\n",
       " 'post',\n",
       " 'guardian',\n",
       " 'npr',\n",
       " 'reuters',\n",
       " 'vox',\n",
       " 'washington',\n",
       " 'post']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs = []\n",
    "for p in df.publication.unique():\n",
    "    pubs.extend(p.lower().split())\n",
    "pubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter most extreme bias publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Breitbart', 'CNN', 'Fox News', 'Talking Points Memo',\n",
       "       'National Review', 'Reuters', 'Vox'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishers = ['Breitbart', 'CNN', 'Fox News', 'Talking Points Memo','National Review','Reuters','Vox']\n",
    "idx = [i for i in range(df.shape[0]) if df.publication.iloc[i] in publishers]\n",
    "df = df.iloc[idx]\n",
    "df.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66697, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual labels from MBFC\n",
    "bias_dict = {\n",
    "    'New York Times': 'left-center',\n",
    "    'Breitbart': 'extreme-right',\n",
    "    'CNN': 'left',\n",
    "    'Business Insider': 'left-center',\n",
    "    'Atlantic': 'left-center',\n",
    "    'Fox News': 'right',\n",
    "    'Talking Points Memo': 'left',\n",
    "    'Buzzfeed News': 'left-center',\n",
    "    'National Review': 'right',\n",
    "    'New York Post': 'right-center',\n",
    "    'Guardian': 'left-center',\n",
    "    'NPR': 'left-center',\n",
    "    'Reuters': 'neutral',\n",
    "    'Vox': 'left',\n",
    "    'Washington Post': 'left-center'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times 1\n",
      "Breitbart -1\n",
      "CNN 1\n",
      "Business Insider 1\n",
      "Atlantic 1\n",
      "Fox News -1\n",
      "Talking Points Memo 1\n",
      "Buzzfeed News 1\n",
      "National Review -1\n",
      "New York Post -1\n",
      "Guardian 1\n",
      "NPR 1\n",
      "Reuters 0\n",
      "Vox 1\n",
      "Washington Post 1\n"
     ]
    }
   ],
   "source": [
    "# simplified labels\n",
    "for k,v in bias_dict.items():\n",
    "    if 'left' in v:\n",
    "        bias_dict[k] = 1\n",
    "    elif 'right' in v:\n",
    "        bias_dict[k] = -1\n",
    "    else:\n",
    "        bias_dict[k] = 0\n",
    "for k,v in bias_dict.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication', 'content'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = [bias_dict[p] for p in df.publication.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication', 'content', 'label'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>23781</td>\n",
       "      <td>23781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>11488</td>\n",
       "      <td>11488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>4354</td>\n",
       "      <td>4354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>6203</td>\n",
       "      <td>6203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>10709</td>\n",
       "      <td>10710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>5213</td>\n",
       "      <td>5214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>4947</td>\n",
       "      <td>4947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     content  label\n",
       "publication                        \n",
       "Breitbart              23781  23781\n",
       "CNN                    11488  11488\n",
       "Fox News                4354   4354\n",
       "National Review         6203   6203\n",
       "Reuters                10709  10710\n",
       "Talking Points Memo     5213   5214\n",
       "Vox                     4947   4947"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('publication').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>34338</td>\n",
       "      <td>34338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10710</td>\n",
       "      <td>10709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21649</td>\n",
       "      <td>21648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication  content\n",
       "label                      \n",
       "-1           34338    34338\n",
       " 0           10710    10709\n",
       " 1           21649    21648"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into liberal and conservative\n",
    "lib = df.loc[df.label == 1]\n",
    "con = df.loc[df.label == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [w.lower() for w in stopwords.words('english')] + \\\n",
    "['trump', 'good','great','bad','pretty','covering','writer',\n",
    " 'author','like','news','follow','tv','said','could','would',\n",
    " 'really','best','journalist','journalists','commentator',\n",
    " 'affiliate','told','reporter','reporters','siriusxm','radio',\n",
    " 'columnist','contributed','bestselling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = [w.lower() for w in names.words('male.txt')]\n",
    "women = [w.lower() for w in names.words('female.txt')]\n",
    "stopNames = men + women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.extend(pubs)\n",
    "stops.extend(stopNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get opinion words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = opinion_lexicon.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stops)\n",
    "ops = set(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7800, 6786)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stops), len(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to pull the n-grams\n",
    "def getNgrams(text, stops=None, ops=None, n=2):\n",
    "    '''return all bigrams in a Counter'''\n",
    "    def criteria(word):\n",
    "        if stops:\n",
    "            if len(word) > 2 and not word.isnumeric() and word not in stops:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    # intialize counter\n",
    "    ngrams = Counter()\n",
    "    \n",
    "    # tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # filter punctuation and single letter words\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    # throw out sentences with less than two words\n",
    "    if len(words) < 2:\n",
    "        return ngrams\n",
    "    \n",
    "    # add bigrams to Counter\n",
    "    for i in range(len(words) - (n-1)):\n",
    "        gram = ' '.join(words[i:i+n])\n",
    "        if all([criteria(w) for w in words[i:i+n]]):\n",
    "            if ops:\n",
    "                if any([w in ops for w in words[i:i+n]]):\n",
    "                    ngrams[gram] += 1\n",
    "            else:\n",
    "                ngrams[gram] += 1\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21649/21649 [03:33<00:00, 101.18it/s]\n"
     ]
    }
   ],
   "source": [
    "libTrigrams = Counter()\n",
    "for pub,content,_ in tqdm(lib.values):\n",
    "    try:\n",
    "        sents = sent_tokenize(content)\n",
    "    except:\n",
    "        sents = []\n",
    "    for text in sents:\n",
    "        libTrigrams.update(getNgrams(text, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21649/21649 [03:44<00:00, 96.26it/s]\n"
     ]
    }
   ],
   "source": [
    "libBigrams = Counter()\n",
    "for pub,content,_ in tqdm(lib.values):\n",
    "    try:\n",
    "        sents = sent_tokenize(content)\n",
    "    except:\n",
    "        sents = []\n",
    "    for text in sents:\n",
    "        libBigrams.update(getNgrams(text, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34338/34338 [04:28<00:00, 127.88it/s]\n"
     ]
    }
   ],
   "source": [
    "conTrigrams = Counter()\n",
    "for pub,content,_ in tqdm(con.values):\n",
    "    try:\n",
    "        sents = sent_tokenize(content)\n",
    "    except:\n",
    "        sents = []\n",
    "    for text in sents:\n",
    "        conTrigrams.update(getNgrams(text, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34338/34338 [04:18<00:00, 132.96it/s]\n"
     ]
    }
   ],
   "source": [
    "conBigrams = Counter()\n",
    "for pub,content,_ in tqdm(con.values):\n",
    "    try:\n",
    "        sents = sent_tokenize(content)\n",
    "    except:\n",
    "        sents = []\n",
    "    for text in sents:\n",
    "        conBigrams.update(getNgrams(text, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1000 most common liberal and conservative bigrams\n",
    "commonConTrigrams = [b[0] for b in conTrigrams.most_common()[:1000]]\n",
    "commonLibTrigrams = [b[0] for b in libTrigrams.most_common()[:1000]]\n",
    "commonConBigrams = [b[0] for b in conBigrams.most_common()[:1000]]\n",
    "commonLibBigrams = [b[0] for b in libBigrams.most_common()[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_bigrams = ['doctor strange','fury road','gold medals','gold medal',\n",
    "                  'mad men','stranger things','little lies','lose weight',\n",
    "                  'world champion','premier league','walking dead',\n",
    "                  'weight loss','grand slam','science fiction','right now.',\n",
    "                  'open champion','associated press','world heavyweight',\n",
    "                  'wonder woman','last month','top stories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the bigrams for each ideology that appear in the top 1000 of that ideology and not in the top 1000 of the other\n",
    "libTrigrams_filtered = [(w, libTrigrams[w]) for w in commonLibTrigrams if w not in commonConTrigrams and not any([b in w for b in remove_bigrams])]\n",
    "libBigrams_filtered = [(w, libBigrams[w]) for w in commonLibBigrams if w not in commonConBigrams and w not in remove_bigrams]\n",
    "# conservative bigrams also had weird 'amp nbsp' HTML tags so remove those\n",
    "conTrigrams_filtered = [(w, conTrigrams[w]) for w in commonConTrigrams if w not in commonLibTrigrams and not any([b in w for b in remove_bigrams])]\n",
    "conBigrams_filtered = [(w, conBigrams[w]) for w in commonConBigrams if w not in commonLibBigrams and w not in remove_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(485, 491, 393, 409)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(libTrigrams_filtered),len(conTrigrams_filtered),len(libBigrams_filtered),len(conBigrams_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 liberal trigrams:\n",
      "senior administration official\n",
      "green card holders\n",
      "greenhouse gas emissions\n",
      "north korean leader\n",
      "federal civil rights\n",
      "provocative narrative essays\n",
      "health care policy\n",
      "republican health care\n",
      "gop health care\n",
      "civil rights laws\n"
     ]
    }
   ],
   "source": [
    "print('top 10 liberal trigrams:')\n",
    "# sorted(libTrigrams_filtered,key=lambda x: -x[1])[:10]\n",
    "for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 liberal bigrams:\n",
      "fast facts\n",
      "opioid epidemic\n",
      "health reform\n",
      "lethal injection\n",
      "budget reconciliation\n",
      "chronic pain\n",
      "lead poisoning\n",
      "intelligence committees\n",
      "prison sentences\n",
      "rights advocates\n"
     ]
    }
   ],
   "source": [
    "print('top 10 liberal bigrams:')\n",
    "# sorted(libBigrams_filtered,key=lambda x: -x[1])[:10]\n",
    "for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 conservative trigrams\n",
      "battleground prediction map\n",
      "jerusalem bureau chief\n",
      "popular weekend talk\n",
      "tweetsa question needing\n",
      "voter suppression cost\n",
      "says voter suppression\n",
      "border patrol agent\n",
      "electionhillary blames america\n",
      "blames america firsthillary\n",
      "america firsthillary says\n"
     ]
    }
   ],
   "source": [
    "print('top 10 conservative trigrams')\n",
    "# sorted(conTrigrams_filtered,key=lambda x: -x[1])[:10]\n",
    "for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 conservative bigrams\n",
      "illegal aliens\n",
      "illegal alien\n",
      "illegal immigrant\n",
      "migrant crisis\n",
      "popular weekend\n",
      "patriot channel\n",
      "hard truths\n",
      "limited government\n",
      "islamic terror\n",
      "twin falls\n"
     ]
    }
   ],
   "source": [
    "print('top 10 conservative bigrams')\n",
    "# sorted(conBigrams_filtered,key=lambda x: -x[1])[:10]\n",
    "for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just the bigrams and drop the count data\n",
    "libGrams = [k for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:100]]\n",
    "conGrams = [k for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just the bigrams and drop the count data\n",
    "with open('../data/libGrams_news5.pickle', 'wb') as f:\n",
    "    pickle.dump(libGrams, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/conGrams_news5.pickle', 'wb') as f:\n",
    "    pickle.dump(conGrams, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load liberal and conservative bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senior administration official', 'green card holders', 'greenhouse gas emissions', 'north korean leader', 'federal civil rights']\n",
      "['battleground prediction map', 'jerusalem bureau chief', 'popular weekend talk', 'tweetsa question needing', 'voter suppression cost']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/libGrams_news5.pickle', 'rb') as f:\n",
    "    libGrams = pickle.load(f)\n",
    "    print(libGrams[:5])\n",
    "with open('../data/conGrams_news5.pickle', 'rb') as f:\n",
    "    conGrams = pickle.load(f)\n",
    "    print(conGrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Data and test sentence filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r'../all-the-news/articles1.csv')\n",
    "df2 = pd.read_csv(r'../all-the-news/articles2.csv')\n",
    "df3 = pd.read_csv(r'../all-the-news/articles3.csv')\n",
    "df = df1.append(df2).append(df3)\n",
    "df.index = range(df.shape[0])\n",
    "df['content'] = df.content + ' ' + df.title\n",
    "df = df.loc[:,['publication','content']]\n",
    "del df1, df2, df3\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual labels from MBFC\n",
    "bias_dict = {\n",
    "    'New York Times': 'left-center',\n",
    "    'Breitbart': 'extreme-right',\n",
    "    'CNN': 'left',\n",
    "    'Business Insider': 'left-center',\n",
    "    'Atlantic': 'left-center',\n",
    "    'Fox News': 'right',\n",
    "    'Talking Points Memo': 'left',\n",
    "    'Buzzfeed News': 'left-center',\n",
    "    'National Review': 'right',\n",
    "    'New York Post': 'right-center',\n",
    "    'Guardian': 'left-center',\n",
    "    'NPR': 'left-center',\n",
    "    'Reuters': 'neutral',\n",
    "    'Vox': 'left',\n",
    "    'Washington Post': 'left-center'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York Times 1\n",
      "Breitbart -1\n",
      "CNN 1\n",
      "Business Insider 1\n",
      "Atlantic 1\n",
      "Fox News -1\n",
      "Talking Points Memo 1\n",
      "Buzzfeed News 1\n",
      "National Review -1\n",
      "New York Post -1\n",
      "Guardian 1\n",
      "NPR 1\n",
      "Reuters 0\n",
      "Vox 1\n",
      "Washington Post 1\n"
     ]
    }
   ],
   "source": [
    "# simplified labels\n",
    "for k,v in bias_dict.items():\n",
    "    if 'left' in v:\n",
    "        bias_dict[k] = 1\n",
    "    elif 'right' in v:\n",
    "        bias_dict[k] = -1\n",
    "    else:\n",
    "        bias_dict[k] = 0\n",
    "for k,v in bias_dict.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = [bias_dict[p] for p in df.publication.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication', 'content', 'label'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepSentence(label, text):\n",
    "    # get bigrams and lexicons\n",
    "    if len(text.split()) < 6:\n",
    "        return False\n",
    "    ngrams = list(getNgrams(text,n=2).keys()) + list(getNgrams(text,n=3).keys())\n",
    "    \n",
    "    # get the bigrams and lexicons that appear in the ideology lists\n",
    "    libNgramSet = set(ngrams).intersection(libGrams)\n",
    "    conNgramSet = set(ngrams).intersection(conGrams)\n",
    "    \n",
    "    # determine whether to keep the sentence\n",
    "    if label == 1:\n",
    "        return libNgramSet\n",
    "    elif label == -1:\n",
    "        return conNgramSet\n",
    "    else:\n",
    "        if libNgramSet or conNgramSet:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterText(df):\n",
    "    filteredText = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        label = df.label.iloc[i]\n",
    "        try:\n",
    "            sents = sent_tokenize(df.content.iloc[i])\n",
    "        except:\n",
    "            sents = []\n",
    "        for text in sents:\n",
    "            ngrams = keepSentence(label,text)\n",
    "            if ngrams:\n",
    "                filteredText.append((label, text, ngrams))\n",
    "            \n",
    "    return pd.DataFrame(filteredText, columns=['label','text','ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.77it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(100)\n",
    "df2 = filterText(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  ngrams\n",
       "label              \n",
       "-1       11      11\n",
       " 0      274     274\n",
       " 1       24      24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s what you need to know: • American divisions are rapidly widening over President Trump’s order to close the U. S. to refugees and people from seven predominantly Muslim countries. \n",
      "\n",
      "Sure enough, she had skin cancer. \n",
      "\n",
      "In a video posted on her campaign’s Facebook page shortly after Mr. Sanders departed the White House grounds to visit the Capitol, Mr. Obama described Mrs. Clinton as the most qualified candidate to seek the White House, and implored Democrats to come together to elect her after a divisive party primary. \n",
      "\n",
      "Doctors realized that excess cholesterol in our blood predicts a higher risk of heart disease. \n",
      "\n",
      "Price, as head of the House Budget Committee and a member of the   Ways and Means Committee, has influence over health care legislation, such as the Affordable Care Act and programs that include Medicare and Medicaid. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df2[df2.label == 1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Friday’s broadcast of HBO’s “Real Time,” filmmaker Michael Moore  reacted to the FBI announcing it was taking a second look into Democratic presidential nominee former Secretary of State Hillary Clinton’s emails by stating, “She has been attacked and abused for 30 years. \n",
      "\n",
      "” Follow Breitbart News investigative reporter and Citizen Journalism School founder Lee Stranahan on Twitter at @Stranahan. \n",
      "\n",
      "And the costs of illegal alien crime continued to mount and a lethal opioid epidemic raged. \n",
      "\n",
      "Obama’s claim of civic peace is also at odds with the televised evidence dramatic race riots,   cop killings, rapes, murders, illegal alien crimes, and chaos that rippled across the country during the second term of his presidency. \n",
      "\n",
      "See the Fox News 2016 battleground prediction map and make your own election projections. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df2[df2.label == -1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump’s national security team is reviewing a wide range of options to counter the missile threat. \n",
      "\n",
      "Rousseff’s survival hinges on winning over a dwindling number of undecided lawmakers who are also being courted by the man poised to take over if she is ousted, Vice President Michel Temer. \n",
      "\n",
      "”The manufacturing malaise that plagued the U. S. is not  . \n",
      "\n",
      "She has branded him a traitor. \n",
      "\n",
      "Showcasing their attempts to unite with other groups for the election, Islamists campaigned with Awdeh Qawwas, a prominent priest, in the affluent Abdoun district of the capital Amman. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df2[df2.label == 0].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142570/142570 [39:11<00:00, 60.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>17969</td>\n",
       "      <td>17969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278353</td>\n",
       "      <td>278353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25901</td>\n",
       "      <td>25901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  ngrams\n",
       "label                \n",
       "-1      17969   17969\n",
       " 0     278353  278353\n",
       " 1      25901   25901"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = filterText(df)\n",
    "df2.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = round(min(df2[df2.label == -1].shape[0], df2[df2.label == 1].shape[0]),-3)\n",
    "n = 17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame,\n",
       " pandas.core.frame.DataFrame,\n",
       " pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = df2.loc[df2.label == -1].sample(n)\n",
    "s2 = df2.loc[df2.label == 1].sample(n)\n",
    "s3 = df2.loc[df2.label == 0].sample(n)\n",
    "type(s1), type(s2), type(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>17000</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17000</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17000</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  ngrams\n",
       "label               \n",
       "-1     17000   17000\n",
       " 0     17000   17000\n",
       " 1     17000   17000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.append(s2).append(s3).groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51000, 3), 51000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = s1.append(s2).append(s3)\n",
    "df2.shape, n*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'ngrams'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'../data/filteredNews_top50grams.pickle', 'wb') as f:\n",
    "#     pickle.dump(df1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'../data/filteredNews5.pickle', 'wb') as f:\n",
    "    pickle.dump(df2.loc[:,['label','text']], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'../data/filteredNews_top50grams.pickle', 'rb') as f:\n",
    "#     df1 = pickle.load(f)\n",
    "with open(r'../data/filteredNews5.pickle', 'rb') as f:\n",
    "    df2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51000, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "label       \n",
       "-1     17000\n",
       " 0     17000\n",
       " 1     17000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('label').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to /home/alex/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time, os, pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, names, opinion_lexicon\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = r'/home/alex/Documents/MIDS/w266/final_project/data/convote_v1.1/data_stage_three'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_set', 'development_set', 'training_set']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "combine the three datasets into one since we are adding it to other datasets later and will do a train test split on the full data at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set : 860\n",
      "development_set : 257\n",
      "training_set : 2740\n",
      "Total files : 3857\n"
     ]
    }
   ],
   "source": [
    "# check how many files we are working with and load them into a list\n",
    "files = []\n",
    "# party2label = {'D':'liberal','R':'conservative','I':'neutral'}\n",
    "party2label = {'D':1,'R':-1,'I':0}\n",
    "\n",
    "# iterate through directory\n",
    "for i, (dirName, subDirList, fileList) in enumerate(os.walk(datapath)):\n",
    "    if i > 0:\n",
    "        # print the number of files in each dataset\n",
    "        print(os.listdir(datapath)[i-1], ':', len(fileList))\n",
    "    for i,f in enumerate(fileList):\n",
    "        # convert from party label to ideological label\n",
    "        label = party2label[f.split('_')[-1][0]]\n",
    "        \n",
    "        # store the filepath with the label attached\n",
    "        filepath = os.path.join(dirName,f)\n",
    "        files.append((label, filepath))\n",
    "print('Total files :', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3857, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse sentences and load into a DataFrame\n",
    "content = []\n",
    "for label, filename in files:\n",
    "    with open(filename, 'r') as f:\n",
    "        # read the content\n",
    "        text = f.read()\n",
    "        \n",
    "        # split the sentences\n",
    "        sents = sent_tokenize(text)\n",
    "        \n",
    "        # collect content into list\n",
    "        content.append((label, text, sents))\n",
    "\n",
    "df = pd.DataFrame(content, columns=['label', 'text', 'sentences'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>-1</td>\n",
       "      <td>mr. speaker , i thank the gentleman for yieldi...</td>\n",
       "      <td>[mr. speaker , i thank the gentleman for yield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>1</td>\n",
       "      <td>mr. chairman , reclaiming the time , i would h...</td>\n",
       "      <td>[mr. chairman , reclaiming the time , i would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>-1</td>\n",
       "      <td>mr. speaker , as members of congress , we have...</td>\n",
       "      <td>[mr. speaker , as members of congress , we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>1</td>\n",
       "      <td>mr. chairman , let me thank the gentleman from...</td>\n",
       "      <td>[mr. chairman , let me thank the gentleman fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>-1</td>\n",
       "      <td>if the gentleman would yield .  i have to remi...</td>\n",
       "      <td>[if the gentleman would yield ., i have to rem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "649      -1  mr. speaker , i thank the gentleman for yieldi...   \n",
       "3778      1  mr. chairman , reclaiming the time , i would h...   \n",
       "3432     -1  mr. speaker , as members of congress , we have...   \n",
       "2521      1  mr. chairman , let me thank the gentleman from...   \n",
       "3703     -1  if the gentleman would yield .  i have to remi...   \n",
       "\n",
       "                                              sentences  \n",
       "649   [mr. speaker , i thank the gentleman for yield...  \n",
       "3778  [mr. chairman , reclaiming the time , i would ...  \n",
       "3432  [mr. speaker , as members of congress , we hav...  \n",
       "2521  [mr. chairman , let me thank the gentleman fro...  \n",
       "3703  [if the gentleman would yield ., i have to rem...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1868</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  sentences\n",
       "label                 \n",
       "-1     1970       1970\n",
       " 0       19         19\n",
       " 1     1868       1868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/convoteFullDocs.pickle', 'wb') as f:\n",
    "    pickle.dump(df.loc[:,['label','sentences']], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine most politically charged bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into liberal and conservative\n",
    "lib = df.loc[df.label == 1]\n",
    "con = df.loc[df.label == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = [w.lower() for w in stopwords.words('english')] + \\\n",
    "        ['``', \"'s\", 'sensenbrenner', 'chairman', \"n't\", \n",
    "         'support', 'extraneous', 'even', 'thank']\n",
    "        \n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = [w.lower() for w in names.words('male.txt')]\n",
    "women = [w.lower() for w in names.words('female.txt')]\n",
    "stopNames = men + women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = opinion_lexicon.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 7944, 6789)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stops), len(stopNames), len(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.extend(stopNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stops)\n",
    "ops = set(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7761, 6786)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stops), len(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function to pull the bigrams\n",
    "def getNgrams(sentence, stops=None, ops=None, n=2):\n",
    "    '''return all bigrams in a Counter'''\n",
    "    def criteria(word):\n",
    "        if stops:\n",
    "            if len(word) > 1 and not word.isnumeric() and word not in stops:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    # intialize counter\n",
    "    ngrams = Counter()\n",
    "    \n",
    "    # tokenize text\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # filter punctuation and single letter words\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    # throw out sentences with less than two words\n",
    "    if len(words) < 2:\n",
    "        return ngrams\n",
    "    \n",
    "    # add bigrams to Counter\n",
    "    for i in range(len(words) - (n-1)):\n",
    "        gram = ' '.join(words[i:i+n])\n",
    "        if all([criteria(w) for w in words[i:i+n]]):\n",
    "            if ops:\n",
    "                if any([w in ops for w in words[i:i+n]]):\n",
    "                    ngrams[gram] += 1\n",
    "            else:\n",
    "                ngrams[gram] += 1\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1868/1868 [00:06<00:00, 301.05it/s]\n"
     ]
    }
   ],
   "source": [
    "libTrigrams = Counter()\n",
    "for sents in tqdm(lib.sentences):\n",
    "    for s in sents:\n",
    "        libTrigrams.update(getNgrams(s, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1868/1868 [00:06<00:00, 279.82it/s]\n"
     ]
    }
   ],
   "source": [
    "libBigrams = Counter()\n",
    "for sents in tqdm(lib.sentences):\n",
    "    for s in sents:\n",
    "        libBigrams.update(getNgrams(s, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1970/1970 [00:05<00:00, 343.62it/s]\n"
     ]
    }
   ],
   "source": [
    "conTrigrams = Counter()\n",
    "for sents in tqdm(con.sentences):\n",
    "    for s in sents:\n",
    "        conTrigrams.update(getNgrams(s, stops=stops, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1970/1970 [00:05<00:00, 349.06it/s]\n"
     ]
    }
   ],
   "source": [
    "conBigrams = Counter()\n",
    "for sents in tqdm(con.sentences):\n",
    "    for s in sents:\n",
    "        conBigrams.update(getNgrams(s, stops=stops, ops=ops, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of these bigrams are neutral. Grab the 1000 most common from each ideology, then filter out any that appear in the other ideology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1000 most common liberal and conservative bigrams\n",
    "commonConTrigrams = [b[0] for b in conTrigrams.most_common()[:1000]]\n",
    "commonLibTrigrams = [b[0] for b in libTrigrams.most_common()[:1000]]\n",
    "commonConBigrams = [b[0] for b in conBigrams.most_common()[:1000]]\n",
    "commonLibBigrams = [b[0] for b in libBigrams.most_common()[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the bigrams for each ideology that appear in the top 1000 of that ideology and not in the top 1000 of the other\n",
    "libTrigrams_filtered = [(w, libTrigrams[w]) for w in commonLibTrigrams if w not in commonConTrigrams]\n",
    "libBigrams_filtered = [(w, libBigrams[w]) for w in commonLibBigrams if w not in commonConBigrams]\n",
    "# conservative bigrams also had weird 'amp nbsp' HTML tags so remove those\n",
    "conTrigrams_filtered = [(w, conTrigrams[w]) for w in commonConTrigrams if w not in commonLibTrigrams]\n",
    "conBigrams_filtered = [(w, conBigrams[w]) for w in commonConBigrams if w not in commonLibBigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 751, 665, 665)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(libTrigrams_filtered),len(conTrigrams_filtered),len(libBigrams_filtered),len(conBigrams_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 liberal trigrams:\n",
      "social security trust\n",
      "security trust fund\n",
      "cbc alternative budget\n",
      "black caucus budget\n",
      "estate tax relief\n",
      "privatize social security\n",
      "u.s. trade deficit\n",
      "republican budget resolution\n",
      "national wildlife refuge\n",
      "guardian ad litem\n"
     ]
    }
   ],
   "source": [
    "print('top 10 liberal trigrams:')\n",
    "# sorted(libTrigrams_filtered,key=lambda x: -x[1])[:10]\n",
    "for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 liberal bigrams:\n",
      "tax breaks\n",
      "security trust\n",
      "bad policy\n",
      "would lose\n",
      "reduce crime\n",
      "budget reconciliation\n",
      "ethical standard\n",
      "fiscally irresponsible\n",
      "working poor\n",
      "subpoena power\n"
     ]
    }
   ],
   "source": [
    "print('top 10 liberal bigrams:')\n",
    "for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 conservative trigrams\n",
      "national electrical contractors\n",
      "electrical contractors association\n",
      "legislative days within\n",
      "inner cell mass\n",
      "head start program\n",
      "community protection act\n",
      "million new jobs\n",
      "death tax repeal\n",
      "9/11 commission report\n",
      "stem cells without\n"
     ]
    }
   ],
   "source": [
    "print('top 10 conservative trigrams')\n",
    "for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 conservative bigrams\n",
      "community protection\n",
      "free market\n",
      "organized crime\n",
      "bankruptcy relief\n",
      "good news\n",
      "relief extension\n",
      "delayed notification\n",
      "soft money\n",
      "illegal aliens\n",
      "invasive species\n"
     ]
    }
   ],
   "source": [
    "print('top 10 conservative bigrams')\n",
    "for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:10]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just the bigrams and drop the count data\n",
    "libGrams = [k for k,v in sorted(libTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(libBigrams_filtered,key=lambda x: -x[1])[:100]]\n",
    "conGrams = [k for k,v in sorted(conTrigrams_filtered,key=lambda x: -x[1])[:100]] + \\\n",
    "           [k for k,v in sorted(conBigrams_filtered,key=lambda x: -x[1])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/libGrams5.pickle', 'wb') as f:\n",
    "    pickle.dump(libGrams, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('../data/conGrams5.pickle', 'wb') as f:\n",
    "    pickle.dump(conGrams, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['social security trust', 'security trust fund', 'cbc alternative budget', 'black caucus budget', 'estate tax relief']\n",
      "['national electrical contractors', 'electrical contractors association', 'legislative days within', 'inner cell mass', 'head start program']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/libGrams5.pickle', 'rb') as f:\n",
    "    libGrams = pickle.load(f)\n",
    "    print(libGrams[:5])\n",
    "with open('../data/conGrams5.pickle', 'rb') as f:\n",
    "    conGrams = pickle.load(f)\n",
    "    print(conGrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(libGrams), len(conGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard look',\n",
       " 'corporate interests',\n",
       " 'critical funding',\n",
       " 'broken promises',\n",
       " 'safe drinking',\n",
       " 'religious persecution',\n",
       " 'work within',\n",
       " 'reconciliation package',\n",
       " 'new debt',\n",
       " 'budget supports']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libGrams[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['necessary reforms',\n",
       " 'another attack',\n",
       " 'wrong message',\n",
       " 'particularly pleased',\n",
       " 'great strides',\n",
       " 'best possible',\n",
       " 'federal criminal',\n",
       " 'adversarial relationship',\n",
       " 'top rate',\n",
       " 'environmental concerns']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conGrams[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data\n",
    "filter the original dataset for only those sentences that contain politically charged bigrams and lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepSentence(label, text):\n",
    "    # get bigrams and lexicons\n",
    "    if len(text.split()) < 6:\n",
    "        return False\n",
    "    ngrams = list(getNgrams(text,n=2).keys()) + list(getNgrams(text,n=3).keys())\n",
    "    \n",
    "    # get the bigrams and lexicons that appear in the ideology lists\n",
    "    libNgramSet = set(ngrams).intersection(libGrams)\n",
    "    conNgramSet = set(ngrams).intersection(conGrams)\n",
    "    \n",
    "    # determine whether to keep the sentence\n",
    "    if label == 1:\n",
    "        return libNgramSet\n",
    "    elif label == -1:\n",
    "        return conNgramSet\n",
    "    else:\n",
    "        if libNgramSet or conNgramSet:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterText(df):\n",
    "    filteredText = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        label, sents = df.label[i], df.sentences[i]\n",
    "        for s in sents:\n",
    "            ngrams = keepSentence(label,s)\n",
    "            if ngrams:\n",
    "                filteredText.append((label, s, ngrams))\n",
    "            \n",
    "    return pd.DataFrame(filteredText, columns=['label','text','ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3857/3857 [00:23<00:00, 162.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_filtered = filterText(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3857, 3), (2922, 3))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1326</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  ngrams\n",
       "label              \n",
       "-1     1326    1326\n",
       " 0      213     213\n",
       " 1     1383    1383"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-1\n",
      " 'and if anyone should doubt that , they only need look at the oil-for-food scandal which my subcommittee , under the leadership of the gentleman from illinois ( mr. hyde )  , has been investigating .'\n",
      " {'oil-for-food scandal'}]\n",
      "1 [1\n",
      " \"it is for that reason that congress passed in 1990 the patient self-determination act as part of obra '90 , which requires all hospitals , long term care facilities , home health agencies , hospice programs and hmos that receive medicare and medicaid dollars to recognize a patient 's living will and power of attorney for health care as advance directives .\"\n",
      " {'long term care'}]\n",
      "2 [-1\n",
      " 'this bill , which provides for federal funding of research using adult stem cells which have , unlike embryonic stem cells , proven medical benefits in treating more than 60 separate diseases , will pass with the overwhelming support of both sides of this debate .'\n",
      " {'using adult stem'}]\n",
      "3 [1\n",
      " 'income among the working poor now fluctuates by as much as 50 percent annually .'\n",
      " {'working poor'}]\n",
      "4 [1\n",
      " 'i urge my colleagues to align their priorities with those of the american people , and vote against the republican budget resolution and for the democratic alternative .'\n",
      " {'republican budget resolution'}]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "sample = df_filtered.sample(n)\n",
    "for i in range(n):\n",
    "    print(i, sample.iloc[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/filteredConvote5.pickle', 'wb') as f:\n",
    "    pickle.dump(df_filtered.loc[:,['label','text']], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>-1</td>\n",
       "      <td>what it merely says is that in an instance whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>-1</td>\n",
       "      <td>in years past , when those of us on the subcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>-1</td>\n",
       "      <td>i have here an april 26 story from the associa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>1</td>\n",
       "      <td>why close the doors to those who are injured b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>-1</td>\n",
       "      <td>finally , i must oppose this bill because it e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1273     -1  what it merely says is that in an instance whe...\n",
       "1337     -1  in years past , when those of us on the subcom...\n",
       "1333     -1  i have here an april 26 story from the associa...\n",
       "2761      1  why close the doors to those who are injured b...\n",
       "1983     -1  finally , i must oppose this bill because it e..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/filteredConvote5.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is clear that there would be plenty of money to deal with the social security trust fund if the president were not using the social security trust fund as a slush fund to give tax cuts to the wealthiest people in america . \n",
      "\n",
      "this bill is another missed opportunity to take america into the future , to take america into the leadership around the world in energy production , energy innovation , and energy technology ; to create a new generation of important products , and a new generation of jobs . \n",
      "\n",
      "instead , these changes will make it harder for people legitimately fleeing persecution to prove their asylum claims and gain protection here . \n",
      "\n",
      "in it , congress provides the yearly resources needed to keep our families healthy , our children educated , our workers employed , and our most vulnerable citizens a productive part of our society . \n",
      "\n",
      "under the business records provision , section 215 of the patriot act , the bill provides that the government may seek a court order for `` any tangible item '' if law enforcement officials assert that the records are sought in an effort to obtain foreign intelligence or in a terrorism investigation . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in test[test.label == 1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you do about private property rights ? \n",
      "\n",
      "on both the business records and delayed notification sections of the patriot act ( among others ) , the stance of the american civil liberties union and like-minded critics seems to have an ulterior motive . \n",
      "\n",
      "it will create a comprehensive national system for sex offender registration , improve information exchange between states when sex offenders move from state to state , and increase penalties for failing to comply with the registration law . \n",
      "\n",
      "i ask members to support the osha reform and in particular h.r. \n",
      "\n",
      "that legislation helped to streamline the intelligence community and tightened some asylum rules that allowed potential terrorists to remain in our country . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in test[test.label == -1].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let us look at what is going on in america today . \n",
      "\n",
      "andy grove , the founder of intel , predicts that the united states will lose the bulk of its information technology to jobs to china and india within the next decade . \n",
      "\n",
      "just yesterday , we learned that general motors is now going to cut back on another 25 , 000 good-paying jobs for american workers . \n",
      "\n",
      "let us pass this resolution . \n",
      "\n",
      "mr. speaker , parliamentary inquiry . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in test[test.label == 0].sample(5).text.values:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
